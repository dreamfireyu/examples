{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78b8254d",
   "metadata": {},
   "source": [
    "# A Full Process Training Guide on CUB-200-2011\n",
    "\n",
    "## 1.Prepare the dataset\n",
    "\n",
    "The **[Caltech-UCSD Birds-200-2011 (CUB-200-2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)** dataset is the most widely-used dataset for fine-grained visual categorization task.\n",
    "* Number of categories: 200\n",
    "* Number of images: 11,788\n",
    "* Annotations per image: 15 Part Locations, 312 Binary Attributes, 1 Bounding Box\n",
    "\n",
    "And here is a [Benchmarks](https://paperswithcode.com/dataset/cub-200-2011) about this dataset you can refer to.\n",
    "First, you should download it to your local path from [Images and annotations](https://drive.google.com/file/d/1hbzc_P1FuxMkcabkgn9ZKinBwW683j45/view).\n",
    "When you downloaded it, you can see this structure.\n",
    "```\n",
    "├── 1.jpg\n",
    "├── 2.jpg\n",
    "├── 3.jpg\n",
    "├── README.md\n",
    "└── data\n",
    "    ├── CUB_200_2011.tgz\n",
    "    └── segmentations.tgz\n",
    "```\n",
    "Just extract `CUB_200_2011.tgz` and rename folder `images` to `images_orig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd data\n",
    "! tar zxvf CUB_200_2011.tgz\n",
    "! cd CUB_200_2011\n",
    "! mv images images_orig\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c342c",
   "metadata": {},
   "source": [
    "**Use this path as your `root_dir`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adef40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/path/to/your/dataset/Birds-200-2011/data/CUB_200_2011'\n",
    "root_dir = '/home/zhangchen/zhangchen_workspace/dataset/caltech-ucsd-bird-200-2011/Caltech-UCSD Birds-200-2011/data/CUB_200_2011'\n",
    "# root_dir = '/Users/zilliz/dataset/Caltech-UCSD Birds-200-2011/data/CUB_200_2011'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f4c344",
   "metadata": {},
   "source": [
    "Loads the image files paths with image_ID (images.txt), and train_test_split.txt designation into Pandas Dataframes for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "orig_images_folder = 'images_orig'\n",
    "new_images_folder = 'images'\n",
    "\n",
    "image_fnames = pd.read_csv(filepath_or_buffer=os.path.join(root_dir, 'images.txt'),\n",
    "                           header=None,\n",
    "                           delimiter=' ',\n",
    "                           names=['Img ID', 'file path'])\n",
    "\n",
    "image_fnames['is training image?'] = pd.read_csv(filepath_or_buffer=os.path.join(root_dir, 'train_test_split.txt'),\n",
    "                                                 header=None, delimiter=' ',\n",
    "                                                 names=['Img ID', 'is training image?'])['is training image?']\n",
    "image_fnames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd84911",
   "metadata": {},
   "source": [
    "Let's modify the dataset files structure to pytorch [ImageFolder](https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) form. Which is a common way of organizing image files in deep learing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6047813",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(root_dir, orig_images_folder)\n",
    "new_data_dir = os.path.join(root_dir, new_images_folder)\n",
    "os.makedirs(os.path.join(new_data_dir, 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(new_data_dir, 'test'), exist_ok=True)\n",
    "\n",
    "for i_image, image_fname in enumerate(image_fnames['file path']):\n",
    "    if image_fnames['is training image?'].iloc[i_image]:\n",
    "        new_dir = os.path.join(new_data_dir, 'train', image_fname.split('/')[0])\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        shutil.copy(src=os.path.join(data_dir, image_fname), dst=os.path.join(new_dir, image_fname.split('/')[1]))\n",
    "    else:\n",
    "        new_dir = os.path.join(new_data_dir, 'test', image_fname.split('/')[0])\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        shutil.copy(src=os.path.join(data_dir, image_fname), dst=os.path.join(new_dir, image_fname.split('/')[1]))\n",
    "print('prepare dataset structure done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab46f6",
   "metadata": {},
   "source": [
    "Using the train_test_split.txt file, each image is copied either to the relevant folder in either the train or test folders. The resulting file will have the following structure:\n",
    "```\n",
    "images-|\n",
    "    train-|\n",
    "        #classname1#-|\n",
    "            image-1.jpg\n",
    "            image-2.jpg\n",
    "        #classname2-|\n",
    "            image-1.jpg\n",
    "            image-2.jpg\n",
    "        |\n",
    "        |\n",
    "        #classnameN-|\n",
    "            image-1.jpg\n",
    "            image-2.jpg\n",
    "    test-|\n",
    "        #classname1#-|\n",
    "            image-1.jpg\n",
    "            image-2.jpg\n",
    "        #classname2-|\n",
    "            image-1.jpg\n",
    "            image-2.jpg\n",
    "        |\n",
    "        |\n",
    "        #classnameN-|\n",
    "            image-1.jpg\n",
    "            image-2.jpg\n",
    "```\n",
    "\n",
    "\n",
    "## 2.Visualise some input data\n",
    "\n",
    "Using some utils in towhee, you can watch the images on your dataset as long as your dataset is in the pytorch [ImageFolder](https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) form of organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3790c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.join(root_dir, 'images')\n",
    "train_data_dir = os.path.join(data_dir, 'train')\n",
    "test_data_dir = os.path.join(data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee.trainer.utils.plot_utils import image_folder_sample_show, image_folder_statistic\n",
    "\n",
    "image_folder_sample_show(train_data_dir, rows=4, cols=4, img_size=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75599592",
   "metadata": {},
   "source": [
    "You can also watch the image count number of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34dfee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee.trainer.utils.plot_utils import image_folder_statistic\n",
    "\n",
    "train_cls_count_dict = image_folder_statistic(train_data_dir, show_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cls_count_dict = image_folder_statistic(test_data_dir, show_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f91484",
   "metadata": {},
   "source": [
    "We found the count number of each class is not balanced, which makes training some difficult.\n",
    "\n",
    "## 3.Setup your training config.\n",
    "\n",
    "Setup your custom training configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee.trainer.training_config import TrainingConfig\n",
    "\n",
    "training_config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9759eed",
   "metadata": {},
   "source": [
    "You can plot the trend chart of lr in the config. The `lr_scheduler_type` can be `linear`, `cosine`, `polynomial`, `constant` or with warmup.\n",
    "\n",
    "#### linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869dfb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee.trainer.utils.plot_utils import plot_lrs_for_config\n",
    "\n",
    "training_config.lr_scheduler_type = 'linear'\n",
    "plot_lrs_for_config(training_config, num_training_steps=100, start_lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571875a",
   "metadata": {},
   "source": [
    "#### cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.lr_scheduler_type = 'cosine'\n",
    "plot_lrs_for_config(training_config, num_training_steps=100, start_lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f456241",
   "metadata": {},
   "source": [
    "#### constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1951182",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.lr_scheduler_type = 'constant'\n",
    "plot_lrs_for_config(training_config, num_training_steps=100, start_lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145590e0",
   "metadata": {},
   "source": [
    "#### cosine with warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb766a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.lr_scheduler_type = 'cosine'\n",
    "training_config.warmup_ratio = 0.1\n",
    "plot_lrs_for_config(training_config, num_training_steps=100, start_lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac906d47",
   "metadata": {},
   "source": [
    "#### custom lr scheduler\n",
    "\n",
    "If you want to use pytorch native lr scheduler to custom your lr, you can use `plot_lrs_for_scheduler()` to previously plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee.trainer.utils.plot_utils import plot_lrs_for_scheduler\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "model = nn.Linear(2, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=100)\n",
    "lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "plot_lrs_for_scheduler(optimizer, lr_scheduler, total_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fbbb1",
   "metadata": {},
   "source": [
    "In this figure, we use `StepLR` in pytorch to custom our lr. If we want to use this lr schedule in towhee, how to config it, we just set `lr_scheduler_type` to a dict, in which `name_` is the class constructor in the `torch.optim.lr_scheduler` module, other key-values are the parameters in this constructor.\n",
    "So we use `plot_lrs_for_config()` to plot the lrs in config, we found the two figures are the same. It means we config it rightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31550ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config.lr_scheduler_type = {\n",
    "    'name_': 'StepLR',\n",
    "    'step_size': 3,\n",
    "    'gamma': 0.1\n",
    "}\n",
    "plot_lrs_for_config(training_config, num_training_steps=10, start_lr=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ad1c7",
   "metadata": {},
   "source": [
    "By this way, the dict in config field can be converted in both directions with yaml. Though we can't configure python object inside yaml file, we can use this method to configure most of the lr scheduler and optimizer.\n",
    "Take optimizer for example, if we want config the custom SGD optimzer, it can be handled in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# equals\n",
    "training_config.optimizer = {\n",
    "    'name_': 'SGD',\n",
    "    'lr': 0.1,\n",
    "    'momentum': 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c3982",
   "metadata": {},
   "source": [
    "For more config and yaml information, see [training_configs]. Now let's specify the training_config in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f775b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = TrainingConfig(\n",
    "    batch_size=16,\n",
    "    epoch_num=50,\n",
    "    device_str='cuda:3',  # if 'cuda', use all of gpus. if 'cuda:0', use No.0 gpu.\n",
    "    dataloader_num_workers=8,\n",
    "    output_dir='cub_200_output',\n",
    "    lr_scheduler_type='cosine',\n",
    "    optimizer='Adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717be99b",
   "metadata": {},
   "source": [
    " In this example, we set epoch_num=50, but it will be early stopped by the default early stop callback with 4 epoch patients. The if you want to use all of gpus in your device, set `device_str=cuda`.You can also use one gpu by specifying the device id, such as `device_str='cuda:2'`.\n",
    "Pay attention, if you train with multi gpus, you should reload the weights from the output dir to do custom test and evaluation.\n",
    "\n",
    "## 4.Define data transforms.\n",
    "\n",
    "Data augmentation can be done during training with data transforms, let's define a `data_transforms` dict about train and test period, where std and mean is calculated from ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8babb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "std = (0.229, 0.224, 0.229)\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "\n",
    "\n",
    "def resizeCropTransforms(img_crop_size=224, img_resize=256):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(img_resize),\n",
    "            transforms.CenterCrop(img_crop_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "            # transforms.RandomHorizontalFlip(p=0.5)\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(img_resize),\n",
    "            transforms.CenterCrop(img_crop_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023534f",
   "metadata": {},
   "source": [
    "Then we can use it to define the training data and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "train_transform = resizeCropTransforms(img_resize=256, img_crop_size=224)['train']\n",
    "val_transform = resizeCropTransforms(img_resize=256, img_crop_size=224)['test']\n",
    "train_data = ImageFolder(train_data_dir, transform=train_transform)\n",
    "eval_data = ImageFolder(test_data_dir, transform=val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72fd1c",
   "metadata": {},
   "source": [
    "We can plot the transforms images to intuitively feel the impact of image transformation on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from towhee.trainer.utils.plot_utils import show_transform\n",
    "\n",
    "img_path = os.path.join(train_data_dir, '118.House_Sparrow', 'House_Sparrow_0006_111034.jpg')\n",
    "\n",
    "show_transform(\n",
    "    image_path=img_path,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean, std)\n",
    "    ]))\n",
    "# show_transform(img_path, resizeCropTransforms()['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38cd4e",
   "metadata": {},
   "source": [
    "## 5. Start to train\n",
    "\n",
    "With all things prepared, you just need to run `train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import towhee\n",
    "\n",
    "op = towhee.ops.towhee.timm_image_embedding(model_name='resnext101_32x8d', num_classes=200)\n",
    "# op2 = towhee.ops.towhee.timm_image_embedding(model_name='resnet152', num_classes=200)\n",
    "\n",
    "op.train(training_config, train_dataset=train_data, eval_dataset=eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b837df",
   "metadata": {},
   "source": [
    "Congratulations, you have successfully trained an operator with towhee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966f5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
